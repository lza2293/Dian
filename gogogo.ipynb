{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "import os\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import gc\n",
    "import math\n",
    "from skimage import transform, io\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, BatchNormalization, Dropout,Activation,Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Img_dir = '../input/dian-set/images/'\n",
    "train_label_dir = '../input/dian-set/labels/training_label'\n",
    "test_label_dir ='../input/dian-set/labels/test_label'\n",
    "words_dir = '../input/dian-set/labels/words'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatedata(label_dir, imgs_dir):\n",
    "    with open(label_dir,'r') as f:\n",
    "        contents = f.readlines() \n",
    "    X = []\n",
    "    for content in contents:\n",
    "        vals = content.split()\n",
    "        curImg_dir = vals[0][:-2] + '00/'\n",
    "        img = io.imread(imgs_dir + curImg_dir + vals[0] + '.jpeg')\n",
    "        img = transform.resize(img, (192, 192))\n",
    "        img = np.array(img)\n",
    "        img = img[8:184,8:184,:]\n",
    "        \n",
    "        X.append(img)\n",
    "    X = np.array(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = generatedata(train_label_dir,Img_dir)\n",
    "\n",
    "np.save('./train_x.npy',X)\n",
    "print('training data generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.read_csv(words_dir, header=None)\n",
    "words.columns = ['features']\n",
    "label_num = 374\n",
    "with open(train_label_dir, 'r') as f:\n",
    "    contents = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_content(contents):\n",
    "    y_ = []\n",
    "    for content in contents:\n",
    "        args = content.split()\n",
    "        cur_label = np.zeros(374)\n",
    "        for i in range(1, len(args)):\n",
    "            cur_label[int(args[i]) - 1] = 1\n",
    "        y_.append(cur_label)\n",
    "    y_ = np.array(y_)\n",
    "    return y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = parse_content(contents)\n",
    "print(labels.shape)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The freq of training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = np.nonzero(labels)[1]\n",
    "freq = np.bincount(freq)\n",
    "print(freq)\n",
    "v = np.arange(0, label_num)\n",
    "sns.barplot(x = v, y = freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get most probable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_probable_index = np.argsort(-freq)\n",
    "print(most_probable_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the propotion of the first most-appeared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_labels = np.sum(freq)\n",
    "count_label = 0\n",
    "for i in range(label_num):\n",
    "    count_label = count_label + freq[most_probable_index[i]]\n",
    "    print('Counting the first {} labels, occupying {}, appear for {} times'.format(i+1, count_label / total_labels, freq[most_probable_index[i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.zeros((4500, label_num)) # 第一个维度是元素数量，第二个维度是是否满足标签\n",
    "for i in range(labels.shape[0]):\n",
    "    cur_label = np.nonzero(labels[i]) # 获取当前图片所有的标签\n",
    "    data[i,cur_label] = 1\n",
    "Data_collection = pd.DataFrame(data = data, dtype = int, columns=range(label_num))\n",
    "print(Data_collection)\n",
    "print(np.sum(Data_collection[4])) # check if it is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop feature and get new_words\n",
    "\n",
    "**new_words** structure : \n",
    "\n",
    "**cur_index**|pre_index|features\n",
    "-|-|-\n",
    "0|0|city\n",
    "...|...|...\n",
    "343|373|hawaii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid_num = 30\n",
    "least_appear = most_probable_index[-rid_num:] - 1\n",
    "new_Data = Data_collection.drop(least_appear, axis=1).columns # 剩下的标签\n",
    "tmp = {'index':new_Data, 'features':words['features'][new_Data]}\n",
    "new_words = pd.DataFrame(tmp)\n",
    "new_words.index = range(label_num - rid_num)\n",
    "new_words.to_csv('./new_words.csv')\n",
    "print(new_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get new labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_data = Data_collection.drop(least_appear, axis=1)\n",
    "new_train_data.columns = range(label_num - rid_num)\n",
    "new_train_data.to_csv('./new_labels.csv')\n",
    "print(new_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./train_y',new_train_data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    input : train_dir : dir of Images\n",
    "    labels : a list of labels with the following format:\n",
    "        [\n",
    "            [Image_index1 label1 label2....]\n",
    "            [Image_index2 label1 label2....]\n",
    "            ...\n",
    "        ]\n",
    "    return a batch of data\n",
    "\"\"\"\n",
    "def getdata(train_dir, labels):\n",
    "    X = []\n",
    "    for label in labels:\n",
    "        args = label.split()\n",
    "        curImg_dir = Img_dir + args[0][:-2] + '00/' + args[0] + '.jpeg'\n",
    "        img = io.imread(curImg_dir)\n",
    "        img = transform.resize(img, (192, 192))\n",
    "        img = np.array(img)\n",
    "        img = img[8:184, 8:184, :]\n",
    "        X.append(img)\n",
    "        \n",
    "    X = np.array(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myModel(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c1 = Conv2D(filters=64, kernel_size=5, padding='valid')  # 卷积层1\n",
    "        self.flatten = Flatten()\n",
    "        self.f3 = Dense(344, activation='softmax')\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        x = self.c1(x)\n",
    "        x = self.flatten(x)\n",
    "        y = self.f3(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def macro_soft_f1(y_true, y_pred):\n",
    "    \"\"\"Compute the macro soft F1-score as a cost (average 1 - soft-F1 across all labels).\n",
    "    Use probability values instead of binary predictions.\n",
    "    \n",
    "    Args:\n",
    "        y (int32 Tensor): targets array of shape (BATCH_SIZE, N_LABELS)\n",
    "        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)\n",
    "        \n",
    "    Returns:\n",
    "        cost (scalar Tensor): value of the cost function for the batch\n",
    "    \"\"\"\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    tp = tf.reduce_sum(y_pred * y_true, axis=0)\n",
    "    fp = tf.reduce_sum(y_pred * (1 - y_true), axis=0)\n",
    "    fn = tf.reduce_sum((1 - y_pred) * y_true, axis=0)\n",
    "    soft_f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "    cost = 1 - soft_f1 # reduce 1 - soft-f1 in order to increase soft-f1\n",
    "    macro_cost = tf.reduce_mean(cost) # average on all labels\n",
    "    return macro_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def myLoss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, dtype = tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    return tf.nn.sigmoid_cross_entropy_with_logits(labels = y_true, logits=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_f1(y_true, y_pred, thresh=0.5):\n",
    "    \"\"\"Compute the macro F1-score on a batch of observations (average F1 across labels)\n",
    "    \n",
    "    Args:\n",
    "        y (int32 Tensor): labels array of shape (BATCH_SIZE, N_LABELS)\n",
    "        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)\n",
    "        thresh: probability value above which we predict positive\n",
    "        \n",
    "    Returns:\n",
    "        macro_f1 (scalar Tensor): value of macro F1 for the batch\n",
    "    \"\"\"\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.cast(tf.greater(y_pred, thresh), tf.float32)\n",
    "    tp = tf.cast(tf.math.count_nonzero(y_pred * y_true), tf.float32)\n",
    "    fp = tf.cast(tf.math.count_nonzero(y_pred * (1 - y_true)), tf.float32)\n",
    "    fn = tf.cast(tf.math.count_nonzero((1 - y_pred) * y_true), tf.float32)\n",
    "    f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_precision(y_true, y_pred, thresh=0.5):\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.cast(tf.greater(y_pred, thresh), tf.float32)\n",
    "    tp = tf.cast(tf.math.count_nonzero(y_pred * y_true), tf.float32)\n",
    "    fp = tf.cast(tf.math.count_nonzero(y_pred * (1 - y_true)), tf.float32)\n",
    "    precision = tp / (tp + fp)\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_recall(y_true, y_pred, thresh=0.5):\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.cast(tf.greater(y_pred, thresh), tf.float32)\n",
    "    tp = tf.cast(tf.math.count_nonzero(y_pred * y_true), tf.float32)\n",
    "    fn = tf.cast(tf.math.count_nonzero((1 - y_pred) * y_true), tf.float32)\n",
    "    recall = tp / (tp + fn)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    data_batch: \n",
    "        img <- getdata(Img_dir, content)\n",
    "        onehot label <- new_train_data.iloc[from:to].values\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_x, train_y, epoch = 1):\n",
    "    batch_size = 32\n",
    "    m = train_x.shape[0]\n",
    "    val_num = 900\n",
    "    seed = 1234\n",
    "    train_num = m - val_num\n",
    "    log_dir = 'checkpoint'\n",
    "    history_path = './history.csv'\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    batch_num = (m - val_num) // batch_size\n",
    "    loop_per_epoch = math.ceil(train_num / (epoch * 4))\n",
    "    \n",
    "    model = myModel()\n",
    "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "    checkpoint = tf.train.Checkpoint(myAwesome = model)\n",
    "    train_db = tf.data.Dataset.from_tensor_slices((train_x, train_y)).shuffle(seed).batch(64).repeat(epoch)\n",
    "    \n",
    "    cur_training_log = pd.DataFrame(columns = ['f1', 'precision', 'recall'])\n",
    "    for i, (x_batch, y_batch) in enumerate(train_db):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(x_batch)\n",
    "            loss = myLoss(y_true = y_batch, y_pred = y_pred)\n",
    "\n",
    "            f1 = get_f1(y_true = y_batch, y_pred = y_pred)\n",
    "            precision = get_precision(y_true = y_batch, y_pred = y_pred)\n",
    "            print('point 2')\n",
    "            recall = get_recall(y_true = y_batch, y_pred = y_pred)\n",
    "\n",
    "            s = pd.Series([f1, precision, recall], index = cur_training_log.columns)\n",
    "            cur_training_log.append(s, ignore_index = True)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "            print('batch finished')\n",
    "        grads = tape.gradient(loss, model.variables)\n",
    "        optimizer.apply_gradients(grads_and_vars = zip(grads, model.variables))\n",
    "        del grads\n",
    "        gc.collect()\n",
    "        if 0 == i % 100:\n",
    "            path = checkpoint.save('./checkpoint/12.ckpt')\n",
    "            print('model saved to %s' % path)\n",
    "            \n",
    "        # TODO: validation set\n",
    "        \n",
    "    if os.path.exists(history_path):\n",
    "        history = pd.read_csv(history_path)\n",
    "    else:\n",
    "        history = pd.DataFrameata(columns = ['f1', 'precision', 'recall'])\n",
    "    \n",
    "    history.append(cur_training_log, ignore_index = True)\n",
    "    history.to_csv(history_path)\n",
    "    \n",
    "    return cur_training_log\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.load('./train_y.npy', allow_pickle = True)\n",
    "x_train = np.load('./train_x.npy', allow_pickle = True)\n",
    "\n",
    "# np.random.seed(1234)\n",
    "# np.random.shuffle(y_train)\n",
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data = train(x_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
